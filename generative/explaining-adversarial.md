(This is a topic in [README.md](../README.md).)
(`TODO`: this file is a stub.)

# Explaining Adversarial Examples

Adversarial methods perturb a given (valid) input by small amount so that it's still the same under human perception, but classified differently by machine. 

*Note* that the term *adversarial* sometimes refers to this infinitesimal perturbation method, while some also uses this term to refer to all artificial counterexamples for ML system. 

There have been a large number of researches done on this topic, such as Papernot et al. [[9]](https://arxiv.org/pdf/1511.07528.pdf).
